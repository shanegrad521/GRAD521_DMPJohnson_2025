# Data Description
# Roles and responsibilities
  My research project focuses on how different wood preservative treatments interact with resin systems, with the goal of improving the durability and performance of engineered wood products. Because this work involves collecting and analyzing large amounts of technical data over time, it’s essential that I manage my data carefully and consistently. Although I’m the only graduate student involved in this project, several key data management roles still need to be addressed, either by me or the Principal Investigator, to ensure the data is reliable, reproducible, and well organized. The PI is responsible for overseeing the overall implementation of the Data Management Plan and ensuring that we meet any institutional, departmental, or funding requirements. I’m responsible for almost all of the hands-on data tasks: setting up and running experiments, collecting data from lab equipment, and documenting everything from treatment conditions to resin performance outcomes. I also manage the organization of the data itself, creating a clear folder structure, using consistent file names, and generating metadata that includes treatment types, sample IDs, dates, and measurements. This is critical for making sure the data stays interpretable over time. Because I’m the only student working on the project, I also handle quality control. I double-check measurements, track instrument calibration, and regularly review my spreadsheets and lab notebooks to make sure everything is accurate. For data analysis, I use a combination of R and Excel to generate summaries, graphs, and comparisons between treatment types. I also maintain a few R scripts that automate parts of the analysis workflow, which I keep documented and organized in a shared drive. Although my project doesn’t involve human subjects, some preservative and resin formulations are proprietary. These data are stored in restricted-access folders managed by the PI to make sure confidential information stays secure. We don’t have formal written documentation assigning roles, but the division of responsibilities is simple and clearly understood between me and my advisor. If I were to leave the project unexpectedly, my files, metadata, and documentation are organized so that someone else could step in and understand what I’ve done. I keep detailed notes on lab procedures, data cleaning steps, and analysis logic to support continuity. Our data is stored in three main places: a lab computer, OSU’s secure research network drive, and my OneDrive account. This setup helps with both day to day access and long-term storage. The OSU network drive is backed up automatically by IT. OneDrive provides a cloud-based off-site backup and syncs automatically across my devices. I also back up the lab computer manually once a week using an external hard drive. Altogether, I follow the 3-2-1 backup strategy: three copies of data, stored on two types of media, with one copy off-site. We are required to comply with NSF’s data management policies due to the source of project funding. This means we need to plan for data preservation, documentation, and eventual sharing of non-sensitive datasets. OSU also has internal guidelines about data security and retention, which we follow. Overall, even though it’s a one-person research team on the student side, I’ve developed a solid data management workflow that supports transparency, reproducibility, and security. It’s a system I can rely on throughout the project, and one that ensures my work is useful not just now, but in the future.
# Data standards and metadata
# Storage and security
# Access and data sharing
# Archiving and preservation
