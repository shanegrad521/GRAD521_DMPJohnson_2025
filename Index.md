# Data Description
# Roles and responsibilities
  My research project focuses on how different wood preservative treatments interact with resin systems, with the goal of improving the durability and performance of engineered wood products. Because this work involves collecting and analyzing large amounts of technical data over time, it’s essential that I manage my data carefully and consistently. Although I’m the only graduate student involved in this project, several key data management roles still need to be addressed, either by me or the Principal Investigator, to ensure the data is reliable, reproducible, and well organized. The PI is responsible for overseeing the overall implementation of the Data Management Plan and ensuring that we meet any institutional, departmental, or funding requirements. I’m responsible for almost all of the hands-on data tasks: setting up and running experiments, collecting data from lab equipment, and documenting everything from treatment conditions to resin performance outcomes. I also manage the organization of the data itself, creating a clear folder structure, using consistent file names, and generating metadata that includes treatment types, sample IDs, dates, and measurements. This is critical for making sure the data stays interpretable over time. Because I’m the only student working on the project, I also handle quality control. I double-check measurements, track instrument calibration, and regularly review my spreadsheets and lab notebooks to make sure everything is accurate. For data analysis, I use a combination of R and Excel to generate summaries, graphs, and comparisons between treatment types. I also maintain a few R scripts that automate parts of the analysis workflow, which I keep documented and organized in a shared drive. Although my project doesn’t involve human subjects, some preservative and resin formulations are proprietary. These data are stored in restricted-access folders managed by the PI to make sure confidential information stays secure. We don’t have formal written documentation assigning roles, but the division of responsibilities is simple and clearly understood between me and my advisor. If I were to leave the project unexpectedly, my files, metadata, and documentation are organized so that someone else could step in and understand what I’ve done. I keep detailed notes on lab procedures, data cleaning steps, and analysis logic to support continuity. Our data is stored in three main places: a lab computer, OSU’s secure research network drive, and my OneDrive account. This setup helps with both day to day access and long-term storage. The OSU network drive is backed up automatically by IT. OneDrive provides a cloud-based off-site backup and syncs automatically across my devices. I also back up the lab computer manually once a week using an external hard drive. Altogether, I follow the 3-2-1 backup strategy: three copies of data, stored on two types of media, with one copy off-site. We are required to comply with NSF’s data management policies due to the source of project funding. This means we need to plan for data preservation, documentation, and eventual sharing of non-sensitive datasets. OSU also has internal guidelines about data security and retention, which we follow. Overall, even though it’s a one-person research team on the student side, I’ve developed a solid data management workflow that supports transparency, reproducibility, and security. It’s a system I can rely on throughout the project, and one that ensures my work is useful not just now, but in the future.
# Data standards and metadata
  For my research project, which looks at how wood preservative treatments interact with phenol-formaldehyde resin in maple wood, organizing and documenting my data well is a big priority. Since I’m the only grad student involved in the project, I’m responsible for every part of the data, from collecting and analyzing it to making sure it’s clearly labeled and documented for future reference. My goal is to have everything easy to understand and traceable later on, both for writing up results and in case I need to revisit the data down the road. Right now, I’m not using GitHub or version control tools outside of what we’ve been introduced to in class. For this project, I’ve been managing my files using a folder system on my computer and my university’s network storage. I have main folders for raw data, processed data, figures, and scripts, with clear subfolders for each experiment. File names follow a consistent format that includes the wood species, preservative type, resin type, and the date, for example, “Maple_CB_PF_060124.csv.” This makes it really easy to know what a file is without opening it. Since I’m not using Git for my R code, I handle versioning manually. Whenever I make updates to a dataset or script, I save a new version with the date or a version number added to the filename, like “RetentionData_v2.csv” or “PreservativeScript_0605.R.” I also keep a short log in a text file where I make quick notes about what’s changed from one version to the next. It’s a basic system, but it works well for now and keeps me from overwriting anything important. As for metadata, I’m not using a formal standard like EML, mostly because it feels too complex for the kind of lab-based work I’m doing. Instead, I’m documenting my data in plain text files that sit next to each dataset. These metadata files include important info like how the data was collected, what variables are included, what units were used, and details about the experimental setup, things like resin concentration, preservative type, temperature, and pressure used during treatment. I also make sure to note any equipment used and the calibration date if that’s relevant. I usually write this documentation right after running an experiment or when I finish entering the data, so I don’t forget any important details. For example, if I’m measuring preservative retention in maple samples treated with copper borate and phenol formaldehyde, I’ll describe how the retention was calculated, what assumptions were made, and why that experiment was run. On top of that, I keep a larger project-level metadata file that includes general information about the whole study, things like the species used, the resin formulations, and an overview of the different treatment combinations I’m testing. While I don’t deal with sensitive data like human subjects, I do work with some proprietary resin and preservative formulations. For anything sensitive, I store it in a restricted folder on the network drive that only my advisor and I can access. Everything else is stored in my general research folders with clear documentation and backup copies. To give an example of how I’m documenting things, I’ve created a simple text file that describes a dataset of retention values from maple samples treated with copper borate and PF resin. It includes the experiment’s purpose, measurement methods, variable names, and units, basically everything someone would need to understand or reuse the data later on. It’s not fancy, but it gets the job done and keeps everything transparent. Overall, my documentation approach is fairly simple, but it’s practical and fits the kind of hands on, lab based research I’m doing. It helps me stay organized, makes writing up results easier, and lays the groundwork for data sharing or archiving when the time comes. I’ll keep refining it as the project moves forward, but so far it’s been working well.
# Storage and security
# Access and data sharing
# Archiving and preservation
